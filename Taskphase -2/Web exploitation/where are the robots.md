# where are the robots
Can you find the robots? https://jupiter.challenges.picoctf.org/problem/60915/ or http://jupiter.challenges.picoctf.org:60915

**FLAG:- picoCTF{ca1cu1at1ng_Mach1n3s_8028f}**

## My approach to this challenge:- 

Opening the website, gives a welcome screen with the text where are the robots. As the text suggests, we can go to `robots.txt` by adding robots.txt in the url path. This gives the user-agent and the paths denied or "disallowed" to the web-crawlers.

![image](https://github.com/user-attachments/assets/092515f9-f514-4e12-ab1f-607982f3119a)

Now we, go to the given html file to get the flag.

![image](https://github.com/user-attachments/assets/7db9889e-3dfe-472e-a84c-4984920bb981)

## Learning Outcomes:-
1. What is `robots.txt` ?
   > The robots.txt file is a simple text file placed on a website to manage and control how search engine crawlers interact with the website. The primary purpose of robots.txt is to instruct web crawlers which parts of the site they are allowed to access and index, and which parts should be avoided.
   > `robots.txt` is typically placed in the root directory of a website.
2. What is a web crawler?
   > A web crawler is a type of automated software used by search engines to browse the internet and collect data from websites.
3. What is user-Agent?
   > User-Agent is a string of text that the browser sends to server to specify various information on the browser's version, device operating system, device type,etc.
4. How `robots.txt` specifies what to allow and what to disallow?
   > It mentions in the user agent what bots / crawlers to allow or disallow. If `*` , it means all bots.
   > The path of file to allow or disallow is placed in allow or disallow category.

## References Used:-

https://developers.google.com/search/docs/crawling-indexing/robots/intro#:~:text=A%20robots.txt%20file%20tells,web%20page%20out%20of%20Google.
